# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WmrxG6XJRFDeiqssmHUdInrJGjHgBoOR

## **Step 1: Enable TPU & Install Dependencies**
"""

import tensorflow as tf
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import ResNet50, VGG16, EfficientNetB0
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input
from tensorflow.keras.optimizers import Adam

# ✅ Detect TPU or fallback to GPU
try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.TPUStrategy(tpu)
    print("✅ Running on TPU v2-8")
except:
    strategy = tf.distribute.MirroredStrategy()
    print("✅ Running on GPU")

from google.colab import drive
drive.mount('/content/drive')

import zipfile

zip_ref = zipfile.ZipFile("/content/drive/MyDrive/RSI-CB128 Dataset.zip", 'r')
zip_ref.extractall("/content/dataset")
zip_ref.close()

!pip install rasterio

# Define Dataset Paths
dataset_path = "/content/dataset"
img_size = (128, 128)
batch_size = 32

# Data Augmentation
train_datagen = ImageDataGenerator(
    rescale=1.0/255,
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    validation_split=0.2  # Splitting into train and validation
)

# Create Train & Validation Generators
train_generator = train_datagen.flow_from_directory(
    dataset_path, target_size=img_size, batch_size=batch_size, class_mode='categorical', subset='training'
)
val_generator = train_datagen.flow_from_directory(
    dataset_path, target_size=img_size, batch_size=batch_size, class_mode='categorical', subset='validation'
)

# Ensure Dataset is Loaded Properly
print(f"Training Samples: {train_generator.samples}")
print(f"Validation Samples: {val_generator.samples}")
print(f"Classes Detected: {train_generator.class_indices}")

# Function to Build Models
def build_model(base_model):
    inputs = Input(shape=(128, 128, 3))
    x = base_model(inputs, training=False)
    x = GlobalAveragePooling2D()(x)
    x = Dense(256, activation='relu')(x)
    outputs = Dense(45, activation='softmax')(x)  # Assign output tensor to 'outputs'
    return Model(inputs, outputs) # Pass 'outputs' to Model constructor

# Train Teacher Models (ResNet50 & VGG16)
with strategy.scope():
    resnet_base = ResNet50(weights="imagenet", include_top=False, input_shape=(128, 128, 3))
    vgg_base = VGG16(weights="imagenet", include_top=False, input_shape=(128, 128, 3))

    resnet_model = build_model(resnet_base)
    vgg_model = build_model(vgg_base)

    resnet_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])
    vgg_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])

    print(" Training ResNet50...")
    resnet_model.fit(train_generator, validation_data=val_generator, epochs=10)

    print(" Training VGG16...")
    vgg_model.fit(train_generator, validation_data=val_generator, epochs=10)

#  Generate Soft Labels
resnet_preds = resnet_model.predict(train_generator)
vgg_preds = vgg_model.predict(train_generator)
alpha = 0.6  # Weighted combination
soft_labels = alpha * resnet_preds + (1 - alpha) * vgg_preds

# Debug Soft Labels
print(" Checking Soft Labels...")
print(f"Min: {np.min(soft_labels)}, Max: {np.max(soft_labels)}")
print(f"Any NaN Values? {np.isnan(soft_labels).any()}")

if np.isnan(soft_labels).any():
    raise ValueError(" Soft labels contain NaN values! Check the teacher model outputs.")

np.save("teacher_soft_labels.npy", soft_labels)

# Train EfficientNetB0 as Student Model
with strategy.scope():
    effnet_base = EfficientNetB0(weights=None, include_top=False, input_shape=(128, 128, 3))
    student_model = build_model(effnet_base)

    def distillation_loss(y_true, y_pred, temperature=2.0):  # Lower temp for stability
        soft_targets = tf.nn.softmax(y_true / temperature)
        student_output = tf.nn.softmax(y_pred / temperature)
        return tf.keras.losses.KLDivergence()(soft_targets, student_output)

    student_model.compile(optimizer=Adam(learning_rate=0.0005), loss=distillation_loss, metrics=['accuracy'])

    print(" Training EfficientNetB0 (Student Model)...")
    student_model.fit(train_generator, validation_data=val_generator, epochs=10)

# Evaluate Models
print("Evaluating Models...")
resnet_loss, resnet_acc = resnet_model.evaluate(val_generator)
print(f"ResNet50 Accuracy: {resnet_acc * 100:.2f}%")

vgg_loss, vgg_acc = vgg_model.evaluate(val_generator)
print(f"VGG16 Accuracy: {vgg_acc * 100:.2f}%")

student_loss, student_acc = student_model.evaluate(val_generator)
print(f"EfficientNetB0 (Student) Accuracy: {student_acc * 100:.2f}%")